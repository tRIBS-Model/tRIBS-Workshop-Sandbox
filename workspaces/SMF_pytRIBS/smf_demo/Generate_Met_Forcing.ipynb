{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554be1e8",
   "metadata": {},
   "source": [
    "# tRIBS-Sandbox: Generate Met Forcing\n",
    "The purpose of this script is to convert the raw observation data from the stations into a format that tRIBS can ingest. For point station data tRIBS requires an meteorological data file (`*.mdf`) and a station descriptor file (`*.sdf`). The first essentially contains meta data for the stations and filepaths to the `*.sdf`. The `*.sdf` contains the actualy time series data.\n",
    "\n",
    "This notebook prepares those files and the required relevant unit conversions. Note that running this script is not required to complete the example but if you would like to chnage the simulation length or model a different storm event entirely then this script will need to be reran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204236ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages we might need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pytRIBS.shared.inout import InOut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2582a",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "Before we start processing our data we will define some functions that we can call to complete the processing we need. Things like execute the pytRIBS functions that write the `*.mdf` and `*.sdf` files or a function that does the general processing of the raw observational data. The functions are not required but they do make the code simpler to apply and read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2deec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meteorological Data: Import Station Data\n",
    "\n",
    "class TribsInputWriter(InOut):\n",
    "    \"\"\"\n",
    "    A lightweight wrapper around pytRIBS InOut to process\n",
    "    observed station data into .mdf and .sdf files.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def process_station_data(self, \n",
    "                             met_df, \n",
    "                             rain_df, \n",
    "                             metadata, \n",
    "                             output_dir, \n",
    "                             file_prefix=\"Station\"):\n",
    "        \"\"\"\n",
    "        Converts a cleaned Pandas DataFrame of observed station data into \n",
    "        tRIBS-compatible MDF and SDF files.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        station_df : pd.DataFrame\n",
    "            DataFrame indexed by datetime. Must contain columns mapped to:\n",
    "            - 'TA': Air Temperature [deg C]\n",
    "            - 'RH': Relative Humidity [%]\n",
    "            - 'PA': Atmospheric Pressure [hPa / mb]\n",
    "            - 'US': Wind Speed [m/s]\n",
    "            - 'IS': Incoming Shortwave Radiation [W/m^2]\n",
    "            - 'R':  Precipitation [mm/hr]\n",
    "        \n",
    "        metadata : dict\n",
    "            Dictionary containing station location info:\n",
    "            {'id': int, 'name': str, 'lat': float, 'lon': float, \n",
    "             'x': float, 'y': float, 'z': float}\n",
    "             \n",
    "        output_dir : str\n",
    "            Path to save the files.\n",
    "            \n",
    "        file_prefix : str\n",
    "            Prefix for filenames (e.g., 'Met_Obs').\n",
    "        \"\"\"\n",
    "        # Prepare File Paths\n",
    "        # Using the station ID in the filename prevents overwriting when processing multiple stations\n",
    "        met_filename = f\"met_{file_prefix}_{metadata['id']}.mdf\"\n",
    "        precip_filename = f\"precip_{file_prefix}_{metadata['id']}.mdf\"\n",
    "        \n",
    "        met_path = os.path.join(output_dir, met_filename)\n",
    "        precip_path = os.path.join(output_dir, precip_filename)\n",
    "        \n",
    "        # Add Dummy Columns for Met if missing\n",
    "        # tRIBS expects specific columns in the .mdf\n",
    "        for col in ['XC', 'TS', 'NR']:\n",
    "            if col not in met_df.columns:\n",
    "                met_df[col] = 9999.99\n",
    "\n",
    "        # Ensure 'date' column exists for the pytRIBS writer\n",
    "        # pytRIBS looks for a column named 'date' to extract Year, Month, Day, Hour\n",
    "        met_df = met_df.copy()\n",
    "        met_df['date'] = met_df.index\n",
    "        \n",
    "        rain_df = rain_df.copy()\n",
    "        rain_df['date'] = rain_df.index\n",
    "\n",
    "        # Write the .mdf Data Files\n",
    "        print(f\"Writing station {metadata['id']} data files to {output_dir}...\")\n",
    "        \n",
    "        # Write Precip (.mdf), This will have 4 rows per hour for 15-min data\n",
    "        self.write_precip_station(rain_df[['R', 'date']].copy(), precip_path) \n",
    "        \n",
    "        # Write Meteorology (.mdf)\n",
    "        met_cols = ['PA', 'RH', 'XC', 'TS', 'NR', 'TA', 'US', 'IS', 'date']\n",
    "        self.write_met_station(met_df[met_cols].copy(), met_path)\n",
    "\n",
    "        # Prepare the Metadata Dictionaries for the .sdf (Header) files\n",
    "        # record_length is simply the number of rows in the .mdf file\n",
    "        met_sdf_entry = {\n",
    "            'station_id': metadata['id'],\n",
    "            'file_path': met_path,\n",
    "            'lat_dd': metadata['lat'],\n",
    "            'long_dd': metadata['lon'],\n",
    "            'x': metadata['x'],\n",
    "            'y': metadata['y'],\n",
    "            'GMT': metadata.get('gmt', -7),\n",
    "            'record_length': len(met_df),\n",
    "            'num_parameters': 12,\n",
    "            'other': metadata['z']\n",
    "        }\n",
    "\n",
    "        precip_sdf_entry = {\n",
    "            'station_id': metadata['id'],\n",
    "            'file_path': precip_path,\n",
    "            'x': metadata['x'],\n",
    "            'y': metadata['y'],\n",
    "            'record_length': len(rain_df),\n",
    "            'num_parameters': 5,\n",
    "            'elevation': metadata['z']\n",
    "        }\n",
    "        \n",
    "        return met_sdf_entry, precip_sdf_entry\n",
    "\n",
    "    def write_sdf_files(self, met_sdf_list, precip_sdf_list, output_dir, file_prefix=\"Station\"):\n",
    "        \"\"\"\n",
    "        Writes the compiled list of station metadata to .sdf files.\n",
    "        \"\"\"\n",
    "        met_sdf_path = os.path.join(output_dir, f\"{file_prefix}_Met.sdf\")\n",
    "        precip_sdf_path = os.path.join(output_dir, f\"{file_prefix}_Precip.sdf\")\n",
    "        \n",
    "        self.write_met_sdf(met_sdf_path, met_sdf_list)\n",
    "        self.write_precip_sdf(precip_sdf_list, precip_sdf_path)\n",
    "        \n",
    "        print(f\"Successfully wrote SDF headers: \\n  {met_sdf_path}\\n  {precip_sdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10999c77",
   "metadata": {},
   "source": [
    "Now that we have our function above defined for processing the data into the correct format for tRIBS we will make a second function that takes our observation data and converts it into the dataframe format required and unit conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb6e47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_raw_observations(excel_path, sheet_mapping, start_date, end_date, freq='h'):\n",
    "    \"\"\"\n",
    "    Reads a multi-sheet Excel file with separate Date/Time columns and \n",
    "    specific headers starting on Row 7.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    excel_path : str\n",
    "        Path to the .xlsx file.\n",
    "    sheet_mapping : dict\n",
    "        Dictionary mapping { 'Excel_Sheet_Name': ('tRIBS_Var', 'Excel_Column_Name') }\n",
    "        \n",
    "        Example: \n",
    "        { \n",
    "          'Precip_Data': ('R', 'Incremental Inches'),\n",
    "          'Temp_Data':   ('TA', 'Avg Temp C') \n",
    "        }\n",
    "        \n",
    "    start_date : str\n",
    "        Start of simulation (e.g., '2010-06-01 00:00:00')\n",
    "    end_date : str\n",
    "        End of simulation.\n",
    "    freq: 'h' for hourly, '15min' for sub-hourly precipitation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Hourly resampled DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"--- Processing File: {os.path.basename(excel_path)} (Freq: {freq}) ---\")\n",
    "    processed_series = []\n",
    "\n",
    "    for sheet_name, (tribs_var, target_col) in sheet_mapping.items():\n",
    "        try:\n",
    "            raw_df = pd.read_excel(excel_path, sheet_name=sheet_name, header=6)\n",
    "            raw_df.columns = [c.strip() for c in raw_df.columns]\n",
    "            \n",
    "            date_col = next((c for c in raw_df.columns if c.lower() == 'date'), None)\n",
    "            time_col = next((c for c in raw_df.columns if c.lower() == 'time'), None)\n",
    "            \n",
    "            dt_index = pd.to_datetime(raw_df[date_col].astype(str) + ' ' + raw_df[time_col].astype(str))\n",
    "            raw_df.set_index(dt_index, inplace=True)\n",
    "            \n",
    "            data_series = raw_df[target_col]\n",
    "\n",
    "            # Use the passed frequency (freq) for resampling\n",
    "            if tribs_var == 'R':\n",
    "                data_resampled = data_series.resample(freq).sum()\n",
    "            else:\n",
    "                data_resampled = data_series.resample('h').mean() # Met is always hourly\n",
    "\n",
    "            data_resampled.name = tribs_var\n",
    "            processed_series.append(data_resampled)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR on sheet '{sheet_name}': {e}\")\n",
    "            continue\n",
    "\n",
    "    df_final = pd.concat(processed_series, axis=1)\n",
    "    df_final = df_final[start_date:end_date]\n",
    "    \n",
    "    if 'R' in df_final.columns:\n",
    "        df_final['R'] = df_final['R'].fillna(0.0)\n",
    "    df_final = df_final.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cb00d",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "Before we cna execute the functions above we need to setup define the required metadata and details how our Excel file with the observations is setup. For this workflow we are going to provide data for the South Mountain Fan and South Mountain Park HQ Gages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing File: SMF_Observations_1993-2025.xlsx (Freq: h) ---\n",
      "--- Processing File: SMF_Observations_1993-2025.xlsx (Freq: 15min) ---\n",
      "--- Processing File: SMF_Observations_1993-2025.xlsx (Freq: 15min) ---\n",
      "Writing station 1 data files to ../smf_init_data/met/...\n",
      "Writing station 2 data files to ../smf_init_data/met/...\n",
      "Successfully wrote SDF headers: \n",
      "  ../smf_init_data/met/Master_Met.sdf\n",
      "  ../smf_init_data/met/Master_Precip.sdf\n"
     ]
    }
   ],
   "source": [
    "start_sim = '2014-08-01 00:00:00'\n",
    "end_sim   = '2014-08-20 23:00:00'\n",
    "output_folder = '../smf_init_data/met/'\n",
    "excel_path = '../smf_init_data/met/SMF_Observations_1993-2025.xlsx'\n",
    "\n",
    "# Variable Name Mapping for South Mountain Fan Gage\n",
    "map_rain_st1 = {\n",
    "    # Sheet Name        # (tRIBS Var,  Excel Column Header)\n",
    "    'SMF Rain':         ('R',          'Incremental inches')\n",
    "}\n",
    "map_weather_st1 = {\n",
    "    # Sheet Name        # (tRIBS Var,  Excel Column Header)\n",
    "    'Temperature':      ('TA',         'Degrees F'),\n",
    "    'Humidity':         ('RH',         'percent'),\n",
    "    'Wind':             ('US',         'miles per hour'),\n",
    "    'Solar Radiation':  ('IS',         'watts/sqm'),\n",
    "    'Pressure':         ('PA',         'millibars')\n",
    "}\n",
    "\n",
    "# Mapping for South Mountain Park HQ Gage, note we are using the same data from SMF gage except for the rainfall\n",
    "map_rain_st2 = {\n",
    "    # Sheet Name        # (tRIBS Var,  Excel Column Header)\n",
    "    'SMPHQ Rain':       ('R',          'Incremental inches')\n",
    "}\n",
    "\n",
    "#  Process Raw Data\n",
    "# Get Met (Hourly)\n",
    "df_st1_met = process_raw_observations(excel_path, map_weather_st1, start_sim, end_sim, freq='h')\n",
    "# Get Rain (15-Min)\n",
    "df_st1_rain = process_raw_observations(excel_path, map_rain_st1, start_sim, end_sim, freq='15min')\n",
    "df_st2_rain = process_raw_observations(excel_path, map_rain_st2, start_sim, end_sim, freq='15min')\n",
    "\n",
    "# Convert Units\n",
    "df_st1_met['TA'] = (df_st1_met['TA'] - 32) * 5/9\n",
    "df_st1_met['US'] = df_st1_met['US'] * 0.44704\n",
    "# Convert Rain Depth (in) to Rate (mm/hr) for 15-min data\n",
    "# Formula: (Inches * 25.4) / 0.25 hours\n",
    "df_st1_rain['R'] = (df_st1_rain['R'] * 25.4) / 0.25\n",
    "df_st2_rain['R'] = (df_st2_rain['R'] * 25.4) / 0.25\n",
    "\n",
    "\n",
    "# GENERATE FILES\n",
    "writer = TribsInputWriter()\n",
    "\n",
    "# Create lists to hold headers for the master files\n",
    "all_met_headers = []\n",
    "all_precip_headers = []\n",
    "\n",
    "# Here we enter in the station metadata that is inserted into the station data file (sdf)\n",
    "meta_station_1 = {\n",
    "    'id': 1,\n",
    "    'lat': 33.31518, 'lon': -112.13369,\n",
    "    'x': 394483, 'y': 3686807, # UTM Zone 12N\n",
    "    'z': 389,   # Elevation in meters\n",
    "    'gmt': -7\n",
    "}\n",
    "## Second Generate the South Mountain Park HQ Gage Files\n",
    "meta_station_2 = {\n",
    "    'id': 2,\n",
    "    'lat': 33.34683, 'lon': -112.08456,\n",
    "    'x': 398949, 'y': 3690196, # UTM Zone 12N\n",
    "    'z': 431,   # Elevation in meters\n",
    "    'gmt': -7\n",
    "}\n",
    "\n",
    "# Process Station 1 and collect metadata\n",
    "m_meta1, p_meta1 = writer.process_station_data(\n",
    "    df_st1_met, df_st1_rain, meta_station_1, output_dir=output_folder, file_prefix=\"SMF\"\n",
    ")\n",
    "all_met_headers.append(m_meta1)\n",
    "all_precip_headers.append(p_meta1)\n",
    "\n",
    "# Process Station 2 and collect metadata\n",
    "m_meta2, p_meta2 = writer.process_station_data(\n",
    "    df_st1_met, df_st2_rain, meta_station_2, output_dir=output_folder, file_prefix=\"SMPHQ\"\n",
    ")\n",
    "all_met_headers.append(m_meta2)\n",
    "all_precip_headers.append(p_meta2)\n",
    "\n",
    "\n",
    "# WRITE MASTER SDF FILES\n",
    "writer.write_sdf_files(all_met_headers, all_precip_headers, output_dir=output_folder, file_prefix=\"Master\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytRIBS)",
   "language": "python",
   "name": "pytribs-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
